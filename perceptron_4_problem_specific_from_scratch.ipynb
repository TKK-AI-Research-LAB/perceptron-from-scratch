{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Perceptron From Scratch — 4 Problem-Specific Implementations\n",
        "\n",
        "This notebook implements and tests four perceptron variants **from scratch** (NumPy only):\n",
        "\n",
        "1) **Binary Perceptron** (linearly separable binary classification)  \n",
        "2) **Pocket Perceptron** (binary classification with non-separable/noisy data)  \n",
        "3) **Kernel Perceptron (RBF)** (nonlinear separations like XOR)  \n",
        "4) **Multiclass Perceptron (joint)** (K-way linear classification)\n",
        "\n",
        "Each section includes:\n",
        "- the node structure (minimal diagram),\n",
        "- what layers exist,\n",
        "- what structural change solves which problem,\n",
        "- an implementation,\n",
        "- experiments and plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0fca8eb4",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:a986c104-9697-4fea-bf71-b09057d54184"
          ]
        }
      ],
      "source": [
        "import torch \n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def train_test_split(X, y, test_size=0.25, shuffle=True, seed=0):\n",
        "    n = X.shape[0]\n",
        "    idx = np.arange(n)\n",
        "    if shuffle:\n",
        "        rng = np.random.default_rng(seed)\n",
        "        rng.shuffle(idx)\n",
        "    cut = int(n * (1 - test_size))\n",
        "    train_idx, test_idx = idx[:cut], idx[cut:]\n",
        "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
        "\n",
        "def standardize_fit(X):\n",
        "    mu = X.mean(axis=0)\n",
        "    sigma = X.std(axis=0)\n",
        "    sigma[sigma == 0] = 1.0\n",
        "    return mu, sigma\n",
        "\n",
        "def standardize_apply(X, mu, sigma):\n",
        "    return (X - mu) / sigma\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return (y_true == y_pred).mean()\n",
        "\n",
        "def plot_2d_points(X, y, title=\"\", ax=None):\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    classes = np.unique(y)\n",
        "    for c in classes:\n",
        "        m = (y == c)\n",
        "        ax.scatter(X[m, 0], X[m, 1], s=25, label=str(c), alpha=0.9)\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "\n",
        "def plot_decision_regions_2d(model, X, y, title=\"\", ax=None, grid_step=0.03):\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    x_min, x_max = X[:, 0].min() - 0.8, X[:, 0].max() + 0.8\n",
        "    y_min, y_max = X[:, 1].min() - 0.8, X[:, 1].max() + 0.8\n",
        "    xs = np.arange(x_min, x_max, grid_step)\n",
        "    ys = np.arange(y_min, y_max, grid_step)\n",
        "    xx, yy = np.meshgrid(xs, ys)\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    preds = model.predict(grid).reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, preds, alpha=0.25, levels=np.unique(preds).size)\n",
        "    plot_2d_points(X, y, title=title, ax=ax)\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Binary Perceptron\n",
        "\n",
        "### Simplest node structure\n",
        "```\n",
        "x1 ─┐\n",
        "x2 ─┼─> [ s = Σ wi*xi + b ] ─> [ sign ] ─> ŷ ∈ {−1,+1}\n",
        "...─┤\n",
        "xd ─┘\n",
        "```\n",
        "\n",
        "### Layers\n",
        "- **Layer 1:** linear score: `s = w·x + b`\n",
        "- **Layer 2:** hard decision: `ŷ = sign(s)`\n",
        "\n",
        "### What problem it targets\n",
        "- Fixed-length vectors → **binary classification** → **one linear boundary** (a hyperplane).\n",
        "\n",
        "### Training rule (core idea)\n",
        "- Only update when the current example is misclassified:\n",
        "  - if `y * (w·x + b) <= 0`:\n",
        "    - `w ← w + η y x`\n",
        "    - `b ← b + η y`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BinaryPerceptron:\n",
        "    def __init__(self, lr=1.0, epochs=50, shuffle=True, seed=0, use_bias=True):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # y must be in {-1, +1}\n",
        "        y = y.astype(int)\n",
        "        assert set(np.unique(y)).issubset({-1, 1})\n",
        "\n",
        "        n, d = X.shape\n",
        "        self.w = np.zeros(d, dtype=float)\n",
        "        self.b = 0.0\n",
        "\n",
        "        rng = np.random.default_rng(self.seed)\n",
        "        self.mistakes_per_epoch = []\n",
        "\n",
        "        for ep in range(self.epochs):\n",
        "            idx = np.arange(n)\n",
        "            if self.shuffle:\n",
        "                rng.shuffle(idx)\n",
        "\n",
        "            mistakes = 0\n",
        "            for i in idx:\n",
        "                s = float(np.dot(self.w, X[i]) + (self.b if self.use_bias else 0.0))\n",
        "                if y[i] * s <= 0:\n",
        "                    self.w += self.lr * y[i] * X[i]\n",
        "                    if self.use_bias:\n",
        "                        self.b += self.lr * y[i]\n",
        "                    mistakes += 1\n",
        "\n",
        "            self.mistakes_per_epoch.append(mistakes)\n",
        "\n",
        "            # separable case early stop: an epoch with 0 mistakes\n",
        "            if mistakes == 0:\n",
        "                break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        return X @ self.w + (self.b if self.use_bias else 0.0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        s = self.decision_function(X)\n",
        "        # tie goes to +1\n",
        "        return np.where(s >= 0, 1, -1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment: linearly separable 2D data\n",
        "\n",
        "We generate two clusters with a clear gap (separable), train the binary perceptron,\n",
        "and plot the decision regions and mistake count per epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_separable_blobs(n=200, gap=2.5, std=0.6, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n1 = n // 2\n",
        "    n2 = n - n1\n",
        "    X1 = rng.normal(loc=(-gap, -gap), scale=std, size=(n1, 2))\n",
        "    X2 = rng.normal(loc=( gap,  gap), scale=std, size=(n2, 2))\n",
        "    X = np.vstack([X1, X2])\n",
        "    y = np.hstack([np.full(n1, -1), np.full(n2, +1)])\n",
        "    return X, y\n",
        "\n",
        "X, y = make_separable_blobs(n=240, gap=2.2, std=0.7, seed=1)\n",
        "\n",
        "# standardize (helps convergence)\n",
        "mu, sigma = standardize_fit(X)\n",
        "Xs = standardize_apply(X, mu, sigma)\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(Xs, y, test_size=0.25, seed=1)\n",
        "\n",
        "model = BinaryPerceptron(lr=1.0, epochs=50, seed=1)\n",
        "model.fit(Xtr, ytr)\n",
        "\n",
        "print(\"Train acc:\", accuracy(ytr, model.predict(Xtr)))\n",
        "print(\"Test  acc:\", accuracy(yte, model.predict(Xte)))\n",
        "print(\"Epochs run:\", len(model.mistakes_per_epoch))\n",
        "print(\"Mistakes per epoch:\", model.mistakes_per_epoch)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "plot_decision_regions_2d(model, Xte, yte, title=\"Binary Perceptron decision regions (test)\", ax=ax[0])\n",
        "ax[1].plot(model.mistakes_per_epoch, marker=\"o\")\n",
        "ax[1].set_title(\"Mistakes per epoch\")\n",
        "ax[1].set_xlabel(\"Epoch\")\n",
        "ax[1].set_ylabel(\"Mistakes\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Pocket Perceptron (non-separable / noisy)\n",
        "\n",
        "### Node structure\n",
        "Same as the binary perceptron (no structural change in the graph).\n",
        "\n",
        "### What changes (to solve a different problem)\n",
        "The standard perceptron may never converge on non-separable data.\n",
        "**Pocket** keeps the **best weights seen so far** (best validation accuracy),\n",
        "so you do not end up with a bad final iterate.\n",
        "\n",
        "### One-sentence training idea\n",
        "- Keep a “pocket” copy of `(w,b)` whenever it improves validation accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PocketPerceptron:\n",
        "    def __init__(self, lr=1.0, epochs=100, shuffle=True, seed=0, use_bias=True, val_split=0.25):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.use_bias = use_bias\n",
        "        self.val_split = val_split\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y = y.astype(int)\n",
        "        assert set(np.unique(y)).issubset({-1, 1})\n",
        "\n",
        "        # internal split for pocket evaluation\n",
        "        Xtr, Xval, ytr, yval = train_test_split(X, y, test_size=self.val_split, seed=self.seed)\n",
        "\n",
        "        n, d = Xtr.shape\n",
        "        self.w = np.zeros(d, dtype=float)\n",
        "        self.b = 0.0\n",
        "\n",
        "        # pocket best\n",
        "        best_w = self.w.copy()\n",
        "        best_b = self.b\n",
        "        best_acc = -1.0\n",
        "\n",
        "        rng = np.random.default_rng(self.seed)\n",
        "        self.val_acc_per_epoch = []\n",
        "        self.train_mistakes_per_epoch = []\n",
        "\n",
        "        for ep in range(self.epochs):\n",
        "            idx = np.arange(n)\n",
        "            if self.shuffle:\n",
        "                rng.shuffle(idx)\n",
        "\n",
        "            mistakes = 0\n",
        "            for i in idx:\n",
        "                s = float(np.dot(self.w, Xtr[i]) + (self.b if self.use_bias else 0.0))\n",
        "                if ytr[i] * s <= 0:\n",
        "                    self.w += self.lr * ytr[i] * Xtr[i]\n",
        "                    if self.use_bias:\n",
        "                        self.b += self.lr * ytr[i]\n",
        "                    mistakes += 1\n",
        "\n",
        "            self.train_mistakes_per_epoch.append(mistakes)\n",
        "\n",
        "            # evaluate on validation\n",
        "            val_pred = self.predict(Xval)\n",
        "            val_acc = accuracy(yval, val_pred)\n",
        "            self.val_acc_per_epoch.append(val_acc)\n",
        "\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                best_w = self.w.copy()\n",
        "                best_b = self.b\n",
        "\n",
        "        # return best\n",
        "        self.w = best_w\n",
        "        self.b = best_b\n",
        "        self.best_val_acc = best_acc\n",
        "        return self\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        return X @ self.w + (self.b if self.use_bias else 0.0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        s = self.decision_function(X)\n",
        "        return np.where(s >= 0, 1, -1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment: noisy binary data (not separable)\n",
        "\n",
        "We create overlapping classes and flip some labels.  \n",
        "Compare **standard** vs **pocket** on test accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_noisy_overlap(n=300, std=1.4, label_flip=0.15, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n1 = n // 2\n",
        "    n2 = n - n1\n",
        "    X1 = rng.normal(loc=(-1.0, -1.0), scale=std, size=(n1, 2))\n",
        "    X2 = rng.normal(loc=( 1.0,  1.0), scale=std, size=(n2, 2))\n",
        "    X = np.vstack([X1, X2])\n",
        "    y = np.hstack([np.full(n1, -1), np.full(n2, +1)])\n",
        "\n",
        "    # flip labels\n",
        "    m = int(label_flip * n)\n",
        "    flip_idx = rng.choice(n, size=m, replace=False)\n",
        "    y[flip_idx] *= -1\n",
        "    return X, y\n",
        "\n",
        "X, y = make_noisy_overlap(n=400, std=1.5, label_flip=0.18, seed=2)\n",
        "mu, sigma = standardize_fit(X)\n",
        "Xs = standardize_apply(X, mu, sigma)\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(Xs, y, test_size=0.3, seed=2)\n",
        "\n",
        "std_model = BinaryPerceptron(lr=1.0, epochs=60, seed=2)\n",
        "std_model.fit(Xtr, ytr)\n",
        "\n",
        "pocket_model = PocketPerceptron(lr=1.0, epochs=60, seed=2, val_split=0.25)\n",
        "pocket_model.fit(Xtr, ytr)\n",
        "\n",
        "print(\"Standard perceptron test acc:\", accuracy(yte, std_model.predict(Xte)))\n",
        "print(\"Pocket perceptron   test acc:\", accuracy(yte, pocket_model.predict(Xte)))\n",
        "print(\"Pocket best val acc:\", pocket_model.best_val_acc)\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
        "plot_decision_regions_2d(std_model, Xte, yte, title=\"Standard perceptron (test)\", ax=ax[0])\n",
        "plot_decision_regions_2d(pocket_model, Xte, yte, title=\"Pocket perceptron (test)\", ax=ax[1])\n",
        "ax[2].plot(pocket_model.val_acc_per_epoch, marker=\"o\", alpha=0.8)\n",
        "ax[2].set_title(\"Pocket: validation accuracy per epoch\")\n",
        "ax[2].set_xlabel(\"Epoch\")\n",
        "ax[2].set_ylabel(\"Val accuracy\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Kernel Perceptron (RBF) — nonlinear separation (XOR)\n",
        "\n",
        "### Node structure\n",
        "Same final decision node, but the “linear score” is replaced by a **kernel score**:\n",
        "\n",
        "```\n",
        "stored examples (xi, yi, αi)  ─┐\n",
        "                              ├─> s(x) = Σ αi yi K(xi, x) + b ─> sign ─> ŷ\n",
        "current x --------------------┘\n",
        "```\n",
        "\n",
        "### What changes (to solve a different problem)\n",
        "A linear perceptron cannot solve XOR in the original input space.  \n",
        "Kernel perceptron replaces `x·x'` with `K(x, x')`, which acts like a nonlinear feature map.\n",
        "\n",
        "### One-sentence training idea\n",
        "- Keep coefficients `αi`; on a mistake, increase the coefficient for that training example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rbf_kernel(X1, X2, gamma=1.0):\n",
        "    # X1: (n1,d), X2: (n2,d)\n",
        "    # returns K: (n1,n2)\n",
        "    # K(x,z) = exp(-gamma * ||x-z||^2)\n",
        "    X1_sq = np.sum(X1**2, axis=1, keepdims=True)\n",
        "    X2_sq = np.sum(X2**2, axis=1, keepdims=True).T\n",
        "    dist2 = X1_sq + X2_sq - 2 * (X1 @ X2.T)\n",
        "    return np.exp(-gamma * dist2)\n",
        "\n",
        "class KernelPerceptronRBF:\n",
        "    def __init__(self, gamma=1.0, lr=1.0, epochs=30, shuffle=True, seed=0, use_bias=True):\n",
        "        self.gamma = gamma\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y = y.astype(int)\n",
        "        assert set(np.unique(y)).issubset({-1, 1})\n",
        "\n",
        "        self.X_train = X.copy()\n",
        "        self.y_train = y.copy()\n",
        "        n = X.shape[0]\n",
        "        self.alpha = np.zeros(n, dtype=float)\n",
        "        self.b = 0.0\n",
        "\n",
        "        # precompute Gram matrix for speed on small datasets\n",
        "        K = rbf_kernel(self.X_train, self.X_train, gamma=self.gamma)\n",
        "\n",
        "        rng = np.random.default_rng(self.seed)\n",
        "        self.mistakes_per_epoch = []\n",
        "\n",
        "        for ep in range(self.epochs):\n",
        "            idx = np.arange(n)\n",
        "            if self.shuffle:\n",
        "                rng.shuffle(idx)\n",
        "\n",
        "            mistakes = 0\n",
        "            for i in idx:\n",
        "                # score using kernel expansion over training points\n",
        "                s = np.sum(self.alpha * self.y_train * K[:, i]) + (self.b if self.use_bias else 0.0)\n",
        "                if self.y_train[i] * s <= 0:\n",
        "                    self.alpha[i] += self.lr\n",
        "                    if self.use_bias:\n",
        "                        self.b += self.lr * self.y_train[i]\n",
        "                    mistakes += 1\n",
        "\n",
        "            self.mistakes_per_epoch.append(mistakes)\n",
        "            if mistakes == 0:\n",
        "                break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        K = rbf_kernel(self.X_train, X, gamma=self.gamma)  # (n_train, n_test)\n",
        "        s = (self.alpha * self.y_train) @ K\n",
        "        if self.use_bias:\n",
        "            s = s + self.b\n",
        "        return s\n",
        "\n",
        "    def predict(self, X):\n",
        "        s = self.decision_function(X)\n",
        "        return np.where(s >= 0, 1, -1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment: XOR (nonlinear) in 2D\n",
        "\n",
        "- Standard perceptron fails because XOR is not linearly separable.\n",
        "- Kernel perceptron with RBF kernel can separate it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_xor(n=300, noise=0.25, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    X = rng.uniform(-1.5, 1.5, size=(n, 2))\n",
        "    y = np.where(X[:, 0] * X[:, 1] >= 0, 1, -1)  # same sign => +1, different => -1\n",
        "    X = X + rng.normal(scale=noise, size=X.shape)\n",
        "    return X, y\n",
        "\n",
        "X, y = make_xor(n=500, noise=0.18, seed=3)\n",
        "mu, sigma = standardize_fit(X)\n",
        "Xs = standardize_apply(X, mu, sigma)\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(Xs, y, test_size=0.3, seed=3)\n",
        "\n",
        "linear = BinaryPerceptron(lr=1.0, epochs=50, seed=3)\n",
        "linear.fit(Xtr, ytr)\n",
        "\n",
        "kernel = KernelPerceptronRBF(gamma=2.0, lr=1.0, epochs=30, seed=3)\n",
        "kernel.fit(Xtr, ytr)\n",
        "\n",
        "print(\"Linear perceptron test acc:\", accuracy(yte, linear.predict(Xte)))\n",
        "print(\"Kernel  perceptron test acc:\", accuracy(yte, kernel.predict(Xte)))\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "plot_decision_regions_2d(linear, Xte, yte, title=\"Linear perceptron on XOR (fails)\", ax=ax[0])\n",
        "plot_decision_regions_2d(kernel, Xte, yte, title=\"Kernel perceptron (RBF) on XOR\", ax=ax[1])\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(kernel.mistakes_per_epoch, marker=\"o\")\n",
        "plt.title(\"Kernel perceptron mistakes per epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Mistakes\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Multiclass Perceptron (joint)\n",
        "\n",
        "### Simplest node structure\n",
        "Instead of one score node, we compute **K scores** and choose the largest:\n",
        "\n",
        "```\n",
        "x -> [ s1 = w1·x + b1 ]\n",
        "x -> [ s2 = w2·x + b2 ]  -> argmax -> class\n",
        "...\n",
        "x -> [ sK = wK·x + bK ]\n",
        "```\n",
        "\n",
        "### Layers\n",
        "- **Layer 1:** K linear score units (one per class)\n",
        "- **Layer 2:** argmax decision\n",
        "\n",
        "### What changes (to solve a different problem)\n",
        "Binary perceptron can only choose between 2 classes.\n",
        "Adding **K score nodes** and an **argmax** makes it work for K-way classification.\n",
        "\n",
        "### One-sentence training idea\n",
        "- If the predicted class is wrong, add `x` to the true class weights and subtract `x` from the predicted class weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MulticlassPerceptron:\n",
        "    def __init__(self, lr=1.0, epochs=60, shuffle=True, seed=0, use_bias=True):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y = y.astype(int)\n",
        "        classes = np.unique(y)\n",
        "        self.classes_ = classes\n",
        "        K = classes.size\n",
        "        n, d = X.shape\n",
        "\n",
        "        # map labels to 0..K-1 internally\n",
        "        self.label_to_idx_ = {c:i for i,c in enumerate(classes)}\n",
        "        self.idx_to_label_ = {i:c for i,c in enumerate(classes)}\n",
        "        y_idx = np.array([self.label_to_idx_[c] for c in y], dtype=int)\n",
        "\n",
        "        self.W = np.zeros((K, d), dtype=float)\n",
        "        self.b = np.zeros(K, dtype=float)\n",
        "\n",
        "        rng = np.random.default_rng(self.seed)\n",
        "        self.mistakes_per_epoch = []\n",
        "\n",
        "        for ep in range(self.epochs):\n",
        "            idx = np.arange(n)\n",
        "            if self.shuffle:\n",
        "                rng.shuffle(idx)\n",
        "\n",
        "            mistakes = 0\n",
        "            for i in idx:\n",
        "                scores = self.W @ X[i] + (self.b if self.use_bias else 0.0)\n",
        "                pred = int(np.argmax(scores))\n",
        "                true = int(y_idx[i])\n",
        "                if pred != true:\n",
        "                    self.W[true] += self.lr * X[i]\n",
        "                    self.W[pred] -= self.lr * X[i]\n",
        "                    if self.use_bias:\n",
        "                        self.b[true] += self.lr\n",
        "                        self.b[pred] -= self.lr\n",
        "                    mistakes += 1\n",
        "\n",
        "            self.mistakes_per_epoch.append(mistakes)\n",
        "            if mistakes == 0:\n",
        "                break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        return X @ self.W.T + (self.b if self.use_bias else 0.0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = self.decision_function(X)\n",
        "        pred_idx = np.argmax(scores, axis=1)\n",
        "        return np.array([self.idx_to_label_[int(i)] for i in pred_idx], dtype=int)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment: 3-class 2D blobs\n",
        "\n",
        "We create three clusters and train the joint multiclass perceptron.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_multiclass_blobs(n=450, std=0.8, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    centers = np.array([[-2.5, 0.0], [2.5, 0.0], [0.0, 2.8]])\n",
        "    K = centers.shape[0]\n",
        "    per = n // K\n",
        "    Xs = []\n",
        "    ys = []\n",
        "    for k in range(K):\n",
        "        Xk = rng.normal(loc=centers[k], scale=std, size=(per, 2))\n",
        "        yk = np.full(per, k, dtype=int)\n",
        "        Xs.append(Xk); ys.append(yk)\n",
        "    X = np.vstack(Xs)\n",
        "    y = np.hstack(ys)\n",
        "    return X, y\n",
        "\n",
        "X, y = make_multiclass_blobs(n=480, std=0.9, seed=4)\n",
        "mu, sigma = standardize_fit(X)\n",
        "Xs = standardize_apply(X, mu, sigma)\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(Xs, y, test_size=0.3, seed=4)\n",
        "\n",
        "mc = MulticlassPerceptron(lr=1.0, epochs=80, seed=4)\n",
        "mc.fit(Xtr, ytr)\n",
        "\n",
        "print(\"Train acc:\", accuracy(ytr, mc.predict(Xtr)))\n",
        "print(\"Test  acc:\", accuracy(yte, mc.predict(Xte)))\n",
        "print(\"Epochs run:\", len(mc.mistakes_per_epoch))\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "plot_decision_regions_2d(mc, Xte, yte, title=\"Multiclass perceptron decision regions (test)\", ax=ax[0])\n",
        "ax[1].plot(mc.mistakes_per_epoch, marker=\"o\")\n",
        "ax[1].set_title(\"Mistakes per epoch\")\n",
        "ax[1].set_xlabel(\"Epoch\")\n",
        "ax[1].set_ylabel(\"Mistakes\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes for education\n",
        "\n",
        "- The **node structure** changes only when the **output format** changes (binary → multiclass) or when the **score computation** changes (linear → kernel).\n",
        "- For noisy data, the **graph stays the same**; what changes is **which weights you keep** (pocket).\n",
        "\n",
        "## Suggested exercises (optional)\n",
        "1) Add a **margin perceptron** by updating when `y*s <= delta`.  \n",
        "2) Add an **averaged perceptron** and compare it to pocket on noisy data.  \n",
        "3) Try different RBF `gamma` on XOR and observe under/overfitting.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
